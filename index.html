<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling Long-Horizon LLM Agent via Context-Folding</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #ffffff;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 80px 0 60px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 700;
            margin-bottom: 30px;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1em;
            margin-bottom: 20px;
            line-height: 1.8;
        }

        .author {
            display: inline-block;
            margin: 0 8px;
        }

        .affiliations {
            font-size: 0.95em;
            margin-top: 20px;
            opacity: 0.95;
            line-height: 1.6;
        }

        .affiliation {
            display: inline-block;
            margin: 0 15px;
        }

        .links {
            margin-top: 30px;
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-block;
            padding: 12px 30px;
            background: white;
            color: #667eea;
            text-decoration: none;
            border-radius: 50px;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.15);
        }

        .btn-secondary {
            background: rgba(255,255,255,0.2);
            color: white;
            backdrop-filter: blur(10px);
        }

        .btn-secondary:hover {
            background: rgba(255,255,255,0.3);
        }

        section {
            padding: 60px 0;
        }

        h2 {
            font-size: 2em;
            margin-bottom: 30px;
            color: #2d3748;
            text-align: center;
        }

        .abstract {
            background: #f7fafc;
            border-left: 4px solid #667eea;
            padding: 30px;
            margin: 30px 0;
            border-radius: 8px;
            font-size: 1.05em;
            line-height: 1.8;
        }

        .highlight-box {
            background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
            padding: 40px;
            border-radius: 12px;
            margin: 40px 0;
        }

        .contributions {
            list-style: none;
            margin: 20px 0;
        }

        .contributions li {
            padding: 15px 15px 15px 50px;
            margin: 10px 0;
            background: white;
            border-radius: 8px;
            position: relative;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        .contributions li:before {
            content: 'âœ“';
            position: absolute;
            left: 20px;
            color: #667eea;
            font-weight: bold;
            font-size: 1.2em;
        }

        .figure-container {
            text-align: center;
            margin: 40px 0;
        }

        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }

        .figure-caption {
            margin-top: 15px;
            font-style: italic;
            color: #666;
        }

        .results-table {
            overflow-x: auto;
            margin: 30px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            border-radius: 8px;
            overflow: hidden;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e2e8f0;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: #f7fafc;
        }

        .method-diagram {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .method-card {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            border-top: 4px solid #667eea;
        }

        .method-card h3 {
            color: #667eea;
            margin-bottom: 15px;
        }

        footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 30px 0;
            margin-top: 60px;
        }

        .citation-box {
            background: #2d3748;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            margin: 20px 0;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .authors {
                font-size: 1em;
            }

            .links {
                flex-direction: column;
                align-items: center;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Scaling Long-Horizon LLM Agent<br>via Context-Folding</h1>

            <div class="authors">
                <span class="author">Weiwei Sun<sup>1,2</sup></span>
                <span class="author">Miao Lu<sup>1,3</sup></span>
                <span class="author">Zhan Ling<sup>1</sup></span>
                <span class="author">Kang Liu<sup>1</sup></span>
                <span class="author">Xuesong Yao<sup>1</sup></span>
                <span class="author">Yiming Yang<sup>2</sup></span>
                <span class="author">Jiecao Chen<sup>1,â€ </sup></span>
            </div>

            <div class="affiliations">
                <span class="affiliation"><sup>1</sup>ByteDance Seed</span>
                <span class="affiliation"><sup>2</sup>Carnegie Mellon University</span>
                <span class="affiliation"><sup>3</sup>Stanford University</span>
            </div>

            <div class="links">
                <a href="#" class="btn">ðŸ“„ Paper (Coming Soon)</a>
                <a href="#" class="btn btn-secondary">ðŸ’» Code (Coming Soon)</a>
                <a href="#" class="btn btn-secondary">ðŸ“Š Dataset</a>
            </div>
        </div>
    </header>

    <main class="container">
        <section id="abstract">
            <h2>Abstract</h2>
            <div class="abstract">
                <p>Large language model (LLM) agents have shown remarkable capabilities in tackling complex, long-horizon problems. However, scaling agents to even longer horizons is fundamentally constrained by the design of agentic frameworks that linearly accumulate the entire interaction history into a single, ever-expanding context.</p>

                <p style="margin-top: 15px;">We propose <strong>Context-Folding</strong>, an agentic mechanism that allows the model to actively manage its working context. The agent can create temporary sub-trajectories for localized subtasks (branch), then summarize and rejoin the main thread (return), with intermediate steps being "folded" away. We also introduce <strong>FoldGRPO</strong>, a novel RL framework with dense, token-level process rewards that trains agents to effectively acquire this capability.</p>

                <p style="margin-top: 15px;">Our Folding Agent achieves <strong>62.0% on BrowseComp-Plus</strong> and <strong>58.0% on SWE-Bench Verified</strong> using only a 32K token budget, surpassing baselines requiring 327K contexts and significantly outperforming summarization-based methods.</p>
            </div>
        </section>

        <section id="key-idea">
            <h2>Key Idea</h2>
            <div class="figure-container">
                <img src="figs/fold_example.png" alt="Context Folding Example" onerror="this.style.display='none'">
                <p class="figure-caption">Figure: Example of an agent folding context in long-horizon tasks. The agent creates branches for token-intensive operations and preserves only key findings.</p>
            </div>
        </section>

        <section id="method">
            <h2>Method</h2>
            <div class="method-diagram">
                <div class="method-card">
                    <h3>Context-Folding Mechanism</h3>
                    <p><strong>Branch:</strong> Create a temporary sub-trajectory for localized subtasks</p>
                    <p style="margin-top: 10px;"><strong>Return:</strong> Summarize outcomes and rejoin the main thread</p>
                    <p style="margin-top: 10px;"><strong>Fold:</strong> Remove intermediate steps, keeping only concise summaries</p>
                </div>

                <div class="method-card">
                    <h3>FoldGRPO Training</h3>
                    <p><strong>Unfolded Token Penalty:</strong> Discourages token-heavy operations in main context</p>
                    <p style="margin-top: 10px;"><strong>Out-of-Scope Penalty:</strong> Maintains focus within sub-tasks</p>
                    <p style="margin-top: 10px;"><strong>Dense Process Rewards:</strong> Provides behavioral guidance on context folding</p>
                </div>
            </div>
        </section>

        <section id="contributions">
            <h2>Key Contributions</h2>
            <div class="highlight-box">
                <ul class="contributions">
                    <li><strong>Context Folding:</strong> A mechanism that enables agents to actively manage their context and mitigate the challenge of linear history growth</li>
                    <li><strong>FoldGRPO:</strong> A reinforcement learning framework with dense process rewards that trains agents to effectively acquire this capability</li>
                    <li><strong>Strong Performance:</strong> Demonstrating promising results on long-horizon benchmarks as a scalable and extensible path toward stronger agency</li>
                </ul>
            </div>
        </section>

        <section id="results">
            <h2>Results</h2>
            <div class="results-table">
                <p style="text-align: center; margin-bottom: 20px;"><strong>Performance on BrowseComp-Plus and SWE-Bench Verified</strong></p>
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Peak Length</th>
                            <th>BrowseComp-Plus</th>
                            <th>SWE-Bench Verified</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #f7fafc;">
                            <td colspan="4"><strong>ReAct Agent with 100B+ LLM</strong></td>
                        </tr>
                        <tr>
                            <td>GPT-5</td>
                            <td>327K</td>
                            <td>79.3%</td>
                            <td>71.8%</td>
                        </tr>
                        <tr>
                            <td>DeepSeek-V3.1</td>
                            <td>327K</td>
                            <td>61.3%</td>
                            <td>61.0%</td>
                        </tr>
                        <tr style="background: #f7fafc;">
                            <td colspan="4"><strong>Folding Agent (Ours) - 36B</strong></td>
                        </tr>
                        <tr>
                            <td>Seed-OSS-36B</td>
                            <td>32K Ã— 10</td>
                            <td>42.0%</td>
                            <td>49.2%</td>
                        </tr>
                        <tr>
                            <td>+ RL (GRPO)</td>
                            <td>32K Ã— 10</td>
                            <td>56.7%</td>
                            <td>56.4%</td>
                        </tr>
                        <tr style="background: #667eea15;">
                            <td><strong>+ RL (FoldGRPO)</strong></td>
                            <td><strong>32K Ã— 10</strong></td>
                            <td><strong>62.0%</strong></td>
                            <td><strong>58.0%</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <div class="citation-box">
@article{sun2025scaling,
  title={Scaling Long-Horizon LLM Agent via Context-Folding},
  author={Sun, Weiwei and Lu, Miao and Ling, Zhan and Liu, Kang and Yao, Xuesong and Yang, Yiming and Chen, Jiecao},
  journal={arXiv preprint},
  year={2025}
}
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 ByteDance Seed. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
